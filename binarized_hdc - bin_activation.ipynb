{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import torch as torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import HD\n",
    "import dataset_utils\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from pytorch_metric_learning import distances, losses, miners, reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binarize_hard(x):\n",
    "    return torch.where(x > 0, 1.0, -1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryModel(nn.Module):\n",
    "    def __init__(self, dim, D, num_classes, enc_type='RP', binary=True, device='cpu', kargs=None):\n",
    "        super(BinaryModel, self).__init__()\n",
    "        self.enc_type, self.binary = enc_type, binary\t\n",
    "        self.device = device\n",
    "        if enc_type in ['RP', 'RP-COS']:\n",
    "            self.rp_layer = nn.Linear(dim, D).to(device)\n",
    "            #self.rp_layer = quant_nn.Linear(dim, D, bias=False, quant_desc_input=tensor_quant.QUANT_DESC_4BIT_PER_TENSOR)\n",
    "            self.class_hvs = torch.zeros(num_classes, D).bool().to(device)\n",
    "            self.class_hvs_nb = torch.zeros(num_classes, D).bool().to(device)\n",
    "        else:\n",
    "            pass\n",
    "    #hard sigmoid    \n",
    "    def weight_binarize(self, W):\n",
    "       W = torch.where(W<-1,-1,W)\n",
    "       W = torch.where(W>1,1,W)\n",
    "       mask1 = (W >= -1) & (W < 0)\n",
    "       W[mask1] = 2 * W[mask1] + W[mask1]*W[mask1]\n",
    "       mask2 = (W >= 0) & (W < 1)\n",
    "       W[mask2] = 2 * W[mask2] - W[mask2]*W[mask2]\n",
    "       # W[W > 1] = 1\n",
    "       return W\n",
    "    #using Bi-Real Approximation     \n",
    "    def activation_binarize(self,a):\n",
    "       a = torch.where(a<-1,-1,a)\n",
    "       a = torch.where(a>1,1,a)\n",
    "       mask1 = (a >= -1) & (a < 0)\n",
    "       a[mask1] = 2 * a[mask1] + a[mask1]*a[mask1]\n",
    "       mask2 = (a >= -0) & (a < 1)\n",
    "       a[mask2] = 2 * a[mask2] - a[mask2]*a[mask2]\n",
    "       #a = torch.where((a >= -1) & (a < 0),2*a + torch.pow(a,2) )\n",
    "       #a = torch.where((a >= 0) & (a < 1), 2*a- torch.pow(a,2))\n",
    "    #    a [a < -1] = -1\n",
    "    #    a [a > 1]   =  1\n",
    "    #    a [(a >= -1) & (a < 0)] = 2*a[(a >= -1) & (a < 0)] + torch.pow(a [(a >= -1) & (a < 0)],2)\n",
    "    #    a [(a >= 0) & (a < 1)] = 2*a[(a >= 0) & (a < 1)] - torch.pow(a [(a >= 0) & (a < 1)],2)\n",
    "       return a\n",
    "\n",
    "    def encoding(self, x):\n",
    "        if self.enc_type == 'RP':\n",
    "            #x = self.activation_binarize(x) \n",
    "            #need not binarize the inputs \n",
    "            #progressively binarize the inputs, after training the weights\n",
    "            #add some print statements and check \n",
    "            #print(\"The value of weights, before binarization\")\n",
    "            print(self.rp_layer.weight.data)\n",
    "            weights = self.rp_layer.weight.data.clone()\n",
    "            weights_bin = self.weight_binarize(weights)\n",
    "            self.rp_layer.weight.data = weights_bin.clone() \n",
    "            \n",
    "            #binarize the input to 4 bits.\n",
    "            \n",
    "            x_bin = self.Quantization(x,4)\n",
    "            out = self.rp_layer(x_bin)\n",
    "            \n",
    "            print(\"The value of weights, after binarization\")\n",
    "            print(self.rp_layer.weight.data)\n",
    "        else:\n",
    "                pass\n",
    "        \n",
    "        return self.activation_binarize(out) if self.binary else out\n",
    "    \n",
    "    #Forward Function\n",
    "    def forward(self, x, embedding=False):\n",
    "        out = self.encoding(x)\n",
    "        if embedding:\n",
    "            out = out\n",
    "        else:\n",
    "            out = self.similarity(class_hvs=binarize_hard(self.class_hvs), enc_hv=out)   \n",
    "        return out\n",
    "    \n",
    "    def init_class(self, x_train, labels_train):\n",
    "        out = self.encoding(x_train)\n",
    "        for i in range(x_train.size()[0]):\n",
    "            self.class_hvs[labels_train[i]] += out[i]\n",
    "\n",
    "        self.class_hvs = binarize_hard(self.class_hvs)\n",
    "        \n",
    "    def HD_train_step(self, x_train, y_train, lr=1.0):\n",
    "        shuffle_idx = torch.randperm(x_train.size()[0])\n",
    "        x_train = x_train[shuffle_idx]\n",
    "        train_labels = y_train[shuffle_idx]\n",
    "        l= list(self.rp_layer.parameters())\n",
    "        enc_hvs = binarize_hard(self.encoding(x_train))\n",
    "        for i in range(enc_hvs.size()[0]):\n",
    "            sims = self.similarity(self.class_hvs, enc_hvs[i].unsqueeze(dim=0))\n",
    "            predict = torch.argmax(sims, dim=1)\n",
    "            \n",
    "            if predict != train_labels[i]:\n",
    "                self.class_hvs_nb[predict] -= lr * enc_hvs[i]\n",
    "                self.class_hvs_nb[train_labels[i]] += lr * enc_hvs[i]\n",
    "            \n",
    "            self.class_hvs.data = binarize_hard(self.class_hvs_nb)\n",
    "    def Quantization(self,X, bits):\n",
    "        alpha = torch.min(X)\n",
    "        beta = torch.max(X)\n",
    "        alpha_q = -2 **(bits -1)\n",
    "        beta_q = 2**(bits-1) -1\n",
    "        s = (beta - alpha)/(beta_q - alpha_q)\n",
    "        z = int((beta * alpha_q - alpha * beta_q) / (beta - alpha))\n",
    "        X_q = torch.round(1/s * X + z,decimals = 0)\n",
    "        X_q = torch.clip(X_q, min = alpha_q, max = beta_q)\n",
    "        return X_q\n",
    "\n",
    "    def similarity(self, class_hvs, enc_hv):\n",
    "\t    return torch.matmul(enc_hv, class_hvs.t())/class_hvs.size()[1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor(np.array([[0.5,2,3],[4,5,6]]))\n",
    "A = torch.where((A >= 0), 1, A)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.tensor([[0.5, 1.2, 0.8],\n",
    "                  [0.1, 0.3, 0.6]])\n",
    "\n",
    "A = torch.min(A)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFeatures, nClasses, x_train, y_train, x_test, y_test, train_loader, test_loader\\\n",
    "        = dataset_utils.load_dataset('mnist', 256, \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = BinaryModel(dim = nFeatures, D=1000, num_classes=nClasses, enc_type='RP', device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "distance = distances.CosineSimilarity()\n",
    "reducer = reducers.ThresholdReducer(low=0.0)\n",
    "loss_func = losses.TripletMarginLoss(margin=0.2, distance=distance, reducer=reducer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0) \n",
    "\n",
    "def HD_test(model, x_test, y_test):\n",
    "    out = model(x_test, embedding=False)\n",
    "    preds = torch.argmax(out, dim=-1)\n",
    "\n",
    "    acc = torch.mean((preds==y_test).float())\t\n",
    "    return acc\n",
    "\n",
    "def get_Hamming_margin(model, x_test, y_test=None):\n",
    "\tdef Hamming_distance(a, b):\n",
    "\t\tD = a.size()[1]\n",
    "\t\treturn (D - a @ b.T)/2\n",
    "\n",
    "\t# Compute mean Hamming distance between class HVS\n",
    "\tclass_hvs = binarize_hard(model.class_hvs.data)\n",
    "\tclass_Hamming_distance = Hamming_distance(class_hvs, class_hvs)\n",
    "\tmean_class_Hamming_distance = torch.mean(class_Hamming_distance).item()\n",
    "\t\n",
    "\n",
    "\t# Compute test samples' Hamming distance\n",
    "\ttest_enc_hvs = binarize_hard(model(x_test, True)) \n",
    "\ttest_Hamming_dist = Hamming_distance(test_enc_hvs, class_hvs)\n",
    "\n",
    "\tsorted_test_Hamming_distance, _ = torch.sort(test_Hamming_dist, dim=-1, descending=False)\n",
    "\ttest_enc_hvs_Hamming_margin = (sorted_test_Hamming_distance[:,1:]-sorted_test_Hamming_distance[:,0].unsqueeze(dim=1)).mean(dim=1).cpu()\n",
    "\tmean_test_Hamming_margin = torch.mean(test_enc_hvs_Hamming_margin).item()\n",
    "\n",
    "\tres_dict = {\n",
    "\t\t\"avg_class_Hamming_dist\": mean_class_Hamming_distance,\n",
    "\t\t\"avg_test_Hamming_margin\": mean_test_Hamming_margin\n",
    "\t}\n",
    "\treturn mean_test_Hamming_margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0149, -0.0243, -0.0272,  ..., -0.0012,  0.0287,  0.0322],\n",
      "        [ 0.0237, -0.0230, -0.0290,  ..., -0.0345,  0.0066, -0.0346],\n",
      "        [ 0.0102, -0.0235,  0.0035,  ...,  0.0350, -0.0101,  0.0110],\n",
      "        ...,\n",
      "        [-0.0026, -0.0240, -0.0022,  ..., -0.0305, -0.0018,  0.0075],\n",
      "        [-0.0135, -0.0133, -0.0174,  ...,  0.0288,  0.0252,  0.0183],\n",
      "        [-0.0224, -0.0060,  0.0228,  ...,  0.0018,  0.0024,  0.0037]])\n",
      "The value of weights, after binarization\n",
      "tensor([[ 0.0296, -0.0481, -0.0536,  ..., -0.0025,  0.0566,  0.0633],\n",
      "        [ 0.0469, -0.0455, -0.0571,  ..., -0.0678,  0.0131, -0.0680],\n",
      "        [ 0.0202, -0.0464,  0.0070,  ...,  0.0688, -0.0200,  0.0218],\n",
      "        ...,\n",
      "        [-0.0052, -0.0474, -0.0044,  ..., -0.0602, -0.0035,  0.0149],\n",
      "        [-0.0269, -0.0265, -0.0345,  ...,  0.0568,  0.0498,  0.0363],\n",
      "        [-0.0444, -0.0120,  0.0451,  ...,  0.0037,  0.0048,  0.0074]])\n",
      "tensor([[ 0.0296, -0.0481, -0.0536,  ..., -0.0025,  0.0566,  0.0633],\n",
      "        [ 0.0469, -0.0455, -0.0571,  ..., -0.0678,  0.0131, -0.0680],\n",
      "        [ 0.0202, -0.0464,  0.0070,  ...,  0.0688, -0.0200,  0.0218],\n",
      "        ...,\n",
      "        [-0.0052, -0.0474, -0.0044,  ..., -0.0602, -0.0035,  0.0149],\n",
      "        [-0.0269, -0.0265, -0.0345,  ...,  0.0568,  0.0498,  0.0363],\n",
      "        [-0.0444, -0.0120,  0.0451,  ...,  0.0037,  0.0048,  0.0074]])\n",
      "The value of weights, after binarization\n",
      "tensor([[ 0.0584, -0.0938, -0.1043,  ..., -0.0050,  0.1101,  0.1226],\n",
      "        [ 0.0916, -0.0888, -0.1109,  ..., -0.1310,  0.0261, -0.1313],\n",
      "        [ 0.0401, -0.0907,  0.0140,  ...,  0.1328, -0.0396,  0.0431],\n",
      "        ...,\n",
      "        [-0.0104, -0.0925, -0.0088,  ..., -0.1167, -0.0070,  0.0295],\n",
      "        [-0.0531, -0.0522, -0.0677,  ...,  0.1104,  0.0971,  0.0713],\n",
      "        [-0.0868, -0.0239,  0.0883,  ...,  0.0074,  0.0097,  0.0147]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_metric_epochs = 2\n",
    "device = \"cpu\"\n",
    "accuracies = []\n",
    "accuracies.append(HD_test(model, x_test, y_test).item())\n",
    "margins = []\n",
    "margins.append(get_Hamming_margin(model, x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.class_hvs = nn.parameter.Parameter(data=model.class_hvs)\n",
    "mining_func = miners.TripletMarginMiner(\n",
    "    margin=0.2, distance=distance, type_of_triplets=\"semihard\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pytorch-metric-learning using Triplet margin loss ###\n",
    "print(\"Triplet Loss\")\n",
    "for epoch_i in range(1, num_metric_epochs + 1):\n",
    "    HD.metric_train(model, loss_func, mining_func, device, train_loader, optimizer, epoch_i)\n",
    "    accuracies.append(HD_test(model, x_test, y_test).item())\n",
    "    margins.append(get_Hamming_margin(model, x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_updated = torch.zeros(model.rp_layer.weight.size())\n",
    "weights_updated = model.rp_layer.weight.data.clone()    \n",
    "weights_updated = binarize_hard(weights_updated)\n",
    "#make the binarize function bipolar\n",
    "model.rp_layer.weight.data  = weights_updated.clone()\n",
    "print(\"Trained Weights\")\n",
    "print(model.rp_layer.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HD_lr = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_HD_epoch = 20\n",
    "for epoch_i in range(1, num_HD_epoch+1):\n",
    "    model.HD_train_step(x_train, y_train, lr=HD_lr)\n",
    "    accuracies.append(HD_test(model, x_test, y_test).item())\n",
    "    margins.append(get_Hamming_margin(model, x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies, label=\"Accuracy\", c = \"blue\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Accuracy of 2 Levels, 1000 Dimensions Dataset:mnist\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"Maximum Accuracy:{}\".format(max(accuracies)))\n",
    "torch.save(model, './model/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.rp_layer.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
